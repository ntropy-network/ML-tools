{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NtropyBenchmark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHO_2EnDZLgR",
        "colab_type": "text"
      },
      "source": [
        "# Combining 4 credit card fraud detection datasets from 3 different organizations into a single classifier \n",
        "\n",
        "The goal of this notebook is to demonstrate the feasibility of combining datasets from different sources with different feature encodings. We will show that a model that has access to multiple datasets of the same kind can be significantly more powerful than a model that is trained on only a single dataset.\n",
        "\n",
        "Let first gain some intuition. The goal of\n",
        "a classifier $f$ trained on a dataset is to find a mapping from the space\n",
        "of features $\\mathcal{X}$ to the space of labels $\\mathcal{Y}$. This can either\n",
        "be done directly, such that $f : \\mathcal{X} \\to \\mathcal{Y}$\n",
        "\n",
        "<img src=\"https://ntropy-figures.s3.us-east-2.amazonaws.com/fig2a.png\" alt=\"Datasets formalism\" width = 400/>\n",
        "\n",
        ", or by first mapping the dataset to some latent space $\\mathcal{Z}$ using a\n",
        "transformation $f_0 : \\mathcal{X} \\to \\mathcal{Z}$, before applying another transformation $f_1 : \\mathcal{Z} \\to \\mathcal{Y}$ to the labels $\\mathcal{Y}$. \n",
        "\n",
        "<img src=\"https://ntropy-figures.s3.us-east-2.amazonaws.com/fig2b.png\" alt=\"Datasets formalism\" width = 400/>\n",
        "\n",
        "With infinite amount of data, $f$ and $g = f_1 \\circ f_2$ are equivalent. However, when information is\n",
        "limited, the transforms have errors, namely $f(x) = y + \\epsilon(x)$ and\n",
        "$g(x) = y + \\mathcal{O}(\\epsilon_0(x) +\n",
        "\\epsilon_1(x))$. For a fixed amount of information, each error grows with the complexity of its respective transform. Equivalently, for a fixed transform complexity, the error decreases with more information.\n",
        "\n",
        "Now, we can pick a $\\mathcal{Z}$, shared across multiple datasets, such that the respective transforms $f_0$ are unique to each dataset, but simple. On the other, the transform $f_1$ is more complex, but the same for all datasets. Now, as it is shared, the more complex $f_1$ can be learned from the combined information of all datasets. The transforms $f_0$ have to be learned using only information in each individual dataset, but are simpler. Hence, the resulting combined error $\\epsilon_0(x) + \\epsilon_1(x))$ can be much smaller than $\\epsilon(x)$ for the original transform $f$.\n",
        "\n",
        "Datasets in the real world that are of similar types, e.g. transactions, individuals, businesses, etc. tend to also have similar fundamental features. It is therefore reasonable to assume that many of them can be mapped to each other with relatively simple transforms and the assumption outlined above holds. Here we will show this is the case on 4 datasets of credit card transactions from 3 different organizations, FICO, Worldline and Vesta.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcnai6lZeLV",
        "colab_type": "text"
      },
      "source": [
        "First, we need to download and extract features from each dataset. Lets define some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSxcqOcRUqsv",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "from os import mkdir, environ\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "def scale(data, a, b):\n",
        "    #scale all values to a range\n",
        "    return MinMaxScaler(feature_range = (a, b)).fit_transform(data)\n",
        "\n",
        "def encode_cols(data, categorical_cols):\n",
        "    #one-hot encode labels and remove outlier categories\n",
        "    for col in categorical_cols:\n",
        "        counts = data[col].value_counts()\n",
        "        max_counts = counts.max()\n",
        "        data[col] = data[col].replace(list(counts[counts < 0.01 * max_counts].keys()), \"OUTLIER\")\n",
        "        has_na = data[col].isna().values.any()\n",
        "        if has_na:\n",
        "            data[col].fillna(\"-\", inplace=True)\n",
        "        data[col] = LabelEncoder().fit_transform(data[col].astype(str))\n",
        "        if not has_na:\n",
        "            data[col] += 1\n",
        "        data[col] = data[col].replace(0, np.nan)\n",
        "    return data\n",
        "\n",
        "#datasets directory    \n",
        "if not exists(\"./datasets\"):\n",
        "    mkdir(\"./datasets\")\n",
        "\n",
        "#Kaggle credentials. To avoid rate limits, please use your own here.\n",
        "environ['KAGGLE_USERNAME'] = \"iliazin\" #Kaggle username\n",
        "environ['KAGGLE_KEY'] = \"d6b031e79ee1df5d927b936a54d44b5c\" #Kaggle api key\n",
        "\n",
        "#install kaggle CLI\n",
        "! pip install -q kaggle --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRqgeREDWJm8",
        "colab_type": "text"
      },
      "source": [
        "### Dataset 1\n",
        "\n",
        "Credit card transactions from Worldline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cko1KCZmYrEn",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "def get_data1():\n",
        "    print(\"data0\")\n",
        "\n",
        "    base_dir = \"./datasets/2_0\"\n",
        "\n",
        "    #load from checkpoint if it exists\n",
        "    if exists(base_dir):\n",
        "        return np.load(base_dir + \"/features.npy\"), np.load(base_dir + \"/labels.npy\")\n",
        "\n",
        "    #download if not found\n",
        "    if not exists(\"creditcardfraud\"):\n",
        "        ! mkdir creditcardfraud\n",
        "        ! kaggle datasets download mlg-ulb/creditcardfraud\n",
        "        ! unzip creditcardfraud.zip -d creditcardfraud\n",
        "\n",
        "    #load into pandas dataframe\n",
        "    data = pd.read_csv(\"creditcardfraud/creditcard.csv\")\n",
        "\n",
        "    #feature extraction\n",
        "    labels = data[\"Class\"]\n",
        "    data = data.drop(columns = [\"Time\", \"Class\"])\n",
        "    data = scale(data, 0, 1)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    #save to checkpoint\n",
        "    mkdir(base_dir)\n",
        "    np.save(base_dir + \"/features.npy\", data)\n",
        "    np.save(base_dir + \"/labels.npy\", labels)\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc8gJh2fY_HX",
        "colab_type": "text"
      },
      "source": [
        "### Dataset 2\n",
        "\n",
        "List of credit card transactions from Vesta Corporation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NKkkPL5ZPgU",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "def get_data2():\n",
        "    print(\"data1\")\n",
        "\n",
        "    base_dir = \"./datasets/2_1\"\n",
        "\n",
        "    #load from checkpoint if it exists\n",
        "    if exists(base_dir):\n",
        "        return np.load(base_dir + \"/features.npy\"), np.load(base_dir + \"/labels.npy\")\n",
        "\n",
        "    #download if not found\n",
        "    if not exists(\"ieee-fraud-detection\"):\n",
        "        ! mkdir ieee-fraud-detection\n",
        "        ! kaggle competitions download -c ieee-fraud-detection\n",
        "        ! unzip train_transaction.csv.zip -d ieee-fraud-detection\n",
        "        ! unzip train_identity.csv.zip -d ieee-fraud-detection\n",
        "\n",
        "    #join both transactions and identities into pandas dataframe\n",
        "    transaction = pd.read_csv(\"ieee-fraud-detection/train_transaction.csv\")\n",
        "    identity = pd.read_csv(\"ieee-fraud-detection/train_identity.csv\")\n",
        "    data = transaction.join(identity.set_index(\"TransactionID\"), on=\"TransactionID\")\n",
        "\n",
        "    #feature engineering\n",
        "    labels = data[\"isFraud\"]\n",
        "    data.drop(columns = [\"isFraud\", \"TransactionID\", \"TransactionDT\"], inplace=True)\n",
        "    categorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\", \n",
        "                   \"P_emaildomain\", \"R_emaildomain\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\", \"M8\", \"M9\", \n",
        "                   \"DeviceType\", \"DeviceInfo\", \"id_12\", \"id_13\", \"id_14\", \"id_15\", \"id_16\", \"id_17\", \"id_18\", \n",
        "                   \"id_19\", \"id_20\", \"id_21\", \"id_22\", \"id_23\", \"id_24\", \"id_25\", \"id_26\", \"id_27\", \"id_28\", \"id_29\", \n",
        "                   \"id_30\", \"id_31\", \"id_32\", \"id_33\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \"id_38\"]\n",
        "\n",
        "    data = encode_cols(data, categorical)\n",
        "    data = scale(data, 1, 2)\n",
        "    data = np.nan_to_num(data)\n",
        "    data = scale(data, 0, 1)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    #save to checkpoint\n",
        "    mkdir(base_dir)\n",
        "    np.save(base_dir + \"/features.npy\", data)\n",
        "    np.save(base_dir + \"/labels.npy\", labels)\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mN83VV6ZUkc",
        "colab_type": "text"
      },
      "source": [
        "### Dataset 3\n",
        "\n",
        "The first dataset form the 2009 UCSD datamining contest. This dataset of credit card transactions was obtained from FICO. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaQwKGDbZTgN",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "def get_data3():\n",
        "    print(\"data2\")\n",
        "\n",
        "    base_dir = \"./datasets/2_2\"\n",
        "\n",
        "    #load from checkpoint if available\n",
        "    if exists(base_dir):\n",
        "        return np.load(base_dir + \"/features.npy\"), np.load(base_dir + \"/labels.npy\")\n",
        "\n",
        "    #download if does not exist\n",
        "    if not exists(\"ucsd-data-mining\"):\n",
        "        ! mkdir ucsd-data-mining\n",
        "        ! wget https://www.cs.purdue.edu/commugrate/data/credit_card/DataminingContest2009.Task1.Train.Inputs.zip\n",
        "        ! wget https://www.cs.purdue.edu/commugrate/data/credit_card/DataminingContest2009.Task1.Train.Targets.zip\n",
        "        ! unzip DataminingContest2009.Task1.Train.Inputs.zip -d ucsd-data-mining\n",
        "        ! unzip DataminingContest2009.Task1.Train.Targets.zip -d ucsd-data-mining\n",
        "\n",
        "    #read into pandas dataframe\n",
        "    data = pd.read_csv(\"ucsd-data-mining/DataminingContest2009.Task1.Train.Inputs\")\n",
        "    labels = pd.read_csv(\"ucsd-data-mining/DataminingContest2009.Task1.Train.Targets\", header = None)\n",
        "\n",
        "    #feature engineering\n",
        "    data.drop(columns = [\"total\"], inplace=True)\n",
        "    categorical = [\"state1\", \"zip1\", \"domain1\"]\n",
        "    data = encode_cols(data, categorical)\n",
        "    data = scale(data, 1, 2)\n",
        "    data = np.nan_to_num(data)\n",
        "    data = scale(data, 0, 1)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels).flatten()\n",
        "\n",
        "    #save to checkpoint\n",
        "    mkdir(base_dir)\n",
        "    np.save(base_dir + \"/features.npy\", data)\n",
        "    np.save(base_dir + \"/labels.npy\", labels)\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSiXCMZ0ZZeG",
        "colab_type": "text"
      },
      "source": [
        "### Dataset 4\n",
        "\n",
        "The second dataset of credit card transactions from the 2009 UCSD datamining contest. This dataset is also from FICO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJLffgf-ZbAF",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "def get_data4():\n",
        "    print(\"data3\")\n",
        "\n",
        "    base_dir = \"./datasets/2_3\"\n",
        "\n",
        "    #load from checkpoint if available\n",
        "    if exists(base_dir):\n",
        "        return np.load(base_dir + \"/features.npy\"), np.load(base_dir + \"/labels.npy\")\n",
        "\n",
        "    #download if files not found\n",
        "    if not exists(\"ucsd-data-mining\"):\n",
        "        ! wget https://www.cs.purdue.edu/commugrate/data/credit_card/DataminingContest2009.Task2.Train.Inputs.zip\n",
        "        ! wget https://www.cs.purdue.edu/commugrate/data/credit_card/DataminingContest2009.Task2.Train.Targets.zip\n",
        "        ! unzip DataminingContest2009.Task2.Train.Inputs.zip -d ucsd-data-mining\n",
        "        ! unzip DataminingContest2009.Task2.Train.Targets.zip -d ucsd-data-mining\n",
        "\n",
        "    #load into pandas dataframe\n",
        "    data = pd.read_csv(\"ucsd-data-mining/DataminingContest2009.Task2.Train.Inputs\")\n",
        "    labels = pd.read_csv(\"ucsd-data-mining/DataminingContest2009.Task2.Train.Targets\", header = None)\n",
        "\n",
        "    #feature engineering\n",
        "    data.drop(columns = [\"total\", \"custAttr1\"], inplace=True)\n",
        "    data[\"custAttr2\"] = data[\"custAttr2\"].apply(lambda x : x.split(\"@\")[1])\n",
        "    categorical = [\"state1\", \"zip1\", \"custAttr2\"]\n",
        "    data = encode_cols(data, categorical)\n",
        "    data = scale(data, 1, 2)\n",
        "    data = np.nan_to_num(data)\n",
        "    data = scale(data, 0, 1)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels).flatten()\n",
        "\n",
        "    #save to checkpoint\n",
        "    mkdir(base_dir)\n",
        "    np.save(base_dir + \"/features.npy\", data)\n",
        "    np.save(base_dir + \"/labels.npy\", labels)\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NVxKwChS5Jy",
        "colab_type": "text"
      },
      "source": [
        "Splitting datasets into train and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cpdFHgLS4O_",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "datasets = [get_data1(), get_data2(), get_data3(), get_data4()]\n",
        "print(\"dataset_dims:\", [data.shape for data, _ in datasets])\n",
        "\n",
        "datasets_train, datasets_test= [], []\n",
        "for data, labels in datasets:\n",
        "    Nc = int(0.5 * len(data))\n",
        "    datasets_train.append((data[:Nc], labels[:Nc]))\n",
        "    datasets_test.append((data[Nc:], labels[Nc:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQRoSU-LaKTi",
        "colab_type": "text"
      },
      "source": [
        "As we are not assuming any shared features between the datasets, we offset each dataset to its own part in the feature vector.\n",
        "\n",
        "<img src=\"https://ntropy-figures.s3.us-east-2.amazonaws.com/fig1.png\" alt=\"Dataset offsets\" width = 500/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LUDEMrTbWKG",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "#to total number of features across all datasets\n",
        "D = sum([data.shape[1] for data, _ in datasets_train])\n",
        "\n",
        "features_train, labels_train = None, None\n",
        "datasets_test_p = []\n",
        "j = 0\n",
        "for i in range(len(datasets_train)):\n",
        "\n",
        "    s = datasets_train[i][0].shape\n",
        "\n",
        "    data_p = np.zeros((s[0],D))\n",
        "    data_p[:,j:j+s[1]] = datasets_train[i][0]\n",
        "    \n",
        "    #decision-tree models can deal with missing values\n",
        "    data_p[data_p==0] = np.nan\n",
        "\n",
        "    #we are training on all datasets at once\n",
        "    if i == 0:\n",
        "        features_train = data_p\n",
        "        labels_train = datasets_train[i][1]\n",
        "    else:\n",
        "        features_train = np.concatenate([features_train, data_p])\n",
        "        labels_train = np.concatenate([labels_train, datasets_train[i][1]])\n",
        "\n",
        "    #the test set needs to be offset in the same way\n",
        "    data_p = np.zeros((datasets_test[i][0].shape[0],D))\n",
        "    data_p[:,j:j+s[1]] = datasets_test[i][0]\n",
        "    data_p[data_p==0] = np.nan\n",
        "\n",
        "    #we are testing performance on each dataset individually\n",
        "    datasets_test_p.append((data_p, datasets_test[i][1]))\n",
        "\n",
        "    j += s[1]\n",
        "dataset_train_p = (features_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkWRYlOBcCoF",
        "colab_type": "text"
      },
      "source": [
        "We comparing performance of a gradient boosting binary classifier on single datasets vs. multiple datasets. We use the [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) metric, which is an industry standard for classifiers. In a production setting, a number of other metrics would be used in parallel, but ROC AUC is a good proxy. **PS: depending on time of day, this takes up to 40 mins to run.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbZTWyF4GzDb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "cellView": "form",
        "outputId": "5b69d361-10c1-4b2a-c104-e75772007633"
      },
      "source": [
        "#@title\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def get_score(train_data, valid_data):\n",
        "    model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', class_weight = 'balanced', learning_rate = 0.05, \n",
        "                               reg_alpha = 0.1, reg_lambda = 0.1, subsample = 0.8, random_state = 50)\n",
        "    model.fit(train_data[0], train_data[1], early_stopping_rounds = 100, eval_metric = 'auc', \n",
        "              eval_set = [valid_data], verbose = False)\n",
        "    pred_valid = model.predict_proba(valid_data[0], num_iteration = model.best_iteration_)[:, 1]\n",
        "    return roc_auc_score(valid_data[1], pred_valid)\n",
        "\n",
        "single, multi = [], []\n",
        "for i in range(len(datasets_train)):\n",
        "    score_single = get_score(datasets_train[i], datasets_test[i])\n",
        "    score_multi = get_score(dataset_train_p, datasets_test_p[i])\n",
        "    single.append(score_single)\n",
        "    multi.append(score_multi)\n",
        "    print(i, score_single, score_multi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.9690573308581625 0.9788612935517076\n",
            "1 0.8877738748135325 0.8882632573687904\n",
            "2 0.79409752793966 0.8356958941515147\n",
            "3 0.8285903329612337 0.8352114429092057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFBtTVmZRKQD",
        "colab_type": "text"
      },
      "source": [
        "We can now plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e0apv2tQ7Eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "cellView": "form",
        "outputId": "c69a7e12-9155-400e-858f-19cfa1151ebe"
      },
      "source": [
        "#@title\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "width = 0.35\n",
        "inds = np.arange(len(single))\n",
        "plt.bar(inds, single, width, label='Single')\n",
        "plt.bar(inds + width, multi, width, label='Multi')\n",
        "plt.ylim([0.98 * min(multi + single), 1.01])\n",
        "plt.ylabel('ROC AUC')\n",
        "plt.xticks(inds + width / 2, [str(i+1) for i in range(len(single))])\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqUOtyLhgOT",
        "colab_type": "text"
      },
      "source": [
        "It is evident that classifier performance on all 4 datasets can be improved by simply combining it with other datasets. For this experiment, the main criteria for picking datasets was public availability and relevant type of labels and features. Training parameters have been picked once without any further tuning. Playing around with learning rate, regularization and other parameters is likely to improve things further. However, as outlined in the start of this notebook, only datasets of similar types and with similar distributions are likely to be \"synergetic\". i.e. MNIST is unlikely to help with catching credit card fraud :)\n",
        "\n",
        "If observations from all datasets are stored in a single database without any privacy or accessibility barriers, a standard classifier, like the one we used above, can easily be trained on the amalgamated data with a few tricks. In reality, however, datasets are scattered across different organizations, which introduces three key complications:\n",
        "\n",
        "*   **Privacy risk** Datasets are often confidential for legal or\n",
        "    commercial reasons. The risk of such a dataset being revealed to a third\n",
        "    party is a liability. This precludes the storing of different\n",
        "    organizations' datasets in one database for easy training.\n",
        "*   **Incompatible feature spaces** Feature encodings are proprietary\n",
        "    to organizations. Disparate datasets will usually have different feature\n",
        "    encodings, even if they contain similar kinds of information. Engineering\n",
        "    effort is traditionally required to reconcile those encodings into a shared\n",
        "    format.\n",
        "*   **Accessibility** It is very unlikely that all datasets will be\n",
        "    accessible at the same time. It is unreasonable to require every\n",
        "    organization to provide a training API that is always available.\n",
        "\n",
        "All three problems can be addressed using a combined model sequentially trained on all datasets in a privacy-preserving way. At Ntropy we are developing such a model, which will soon be accessible through our [Python SDK](https://github.com/ntropy-network/ntropy-client).\n",
        "\n",
        "If you have any questions or comments, please do not hesitate to contact us at hello@ntropy.network. We are also [hiring](https://docs.google.com/document/u/1/d/1B0kMa5Gvnq8PIIGNtbk0KDguvRFnGXgJGxjHJUpbpRI)."
      ]
    }
  ]
}
